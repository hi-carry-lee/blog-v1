import {
  generateEmbedding,
  batchGenerateEmbeddings,
  chunkText,
  countTokens,
  MAX_TOKENS,
} from "./embedding";
import {
  insertEmbedding,
  batchInsertEmbeddings,
  deleteEmbeddingsByPostId,
  searchSimilarEmbeddings,
} from "../vector";

export interface Post {
  id: string;
  title: string;
  content: string;
}

/**
 * ä¸ºæ–‡ç« ç”Ÿæˆæ‰€æœ‰ embeddings
 */
export async function generatePostEmbeddings(post: Post) {
  try {
    const { id, title, content } = post;

    // 1. ç”Ÿæˆæ ‡é¢˜ embeddingï¼ˆæ€»æ˜¯å•ç‹¬ä¸€æ¡ï¼‰
    const titleEmbedding = await generateEmbedding(title);
    const titleTokens = countTokens(title);

    await insertEmbedding({
      postId: id,
      contentType: "title",
      textChunk: title,
      embedding: titleEmbedding,
      tokenCount: titleTokens,
    });

    // 2. å¤„ç†æ­£æ–‡ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å—
    const contentTokens = countTokens(content);

    if (contentTokens <= MAX_TOKENS) {
      // æ­£æ–‡è¾ƒçŸ­ï¼Œæ•´ä½“ embed
      const contentEmbedding = await generateEmbedding(content);
      await insertEmbedding({
        postId: id,
        contentType: "content",
        textChunk: content,
        embedding: contentEmbedding,
        tokenCount: contentTokens,
      });
    } else {
      // æ­£æ–‡è¾ƒé•¿ï¼Œéœ€è¦åˆ†å—
      console.log(`ğŸ“š Post ${id} is long, chunking...`);
      const chunks = chunkText(content, {
        maxTokens: 500,
        overlap: 50,
      });

      // æ‰¹é‡ç”Ÿæˆ embeddingsï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
      const chunkTexts = chunks.map((c) => c.text);
      const chunkEmbeddings = await batchGenerateEmbeddings(chunkTexts);

      // æ‰¹é‡æ’å…¥æ•°æ®åº“
      await batchInsertEmbeddings(
        chunks.map((chunk, index) => ({
          postId: id,
          contentType: "chunk" as const,
          textChunk: chunk.text,
          embedding: chunkEmbeddings[index],
          chunkIndex: chunk.index,
          tokenCount: chunk.tokenCount,
        }))
      );

      console.log(`âœ… Generated ${chunks.length} chunks for post ${id}`);
    }
  } catch (error) {
    console.error(`Failed to generate embeddings for post ${post.id}:`, error);
    throw error;
  }
}

/**
 * æ›´æ–°æ–‡ç« çš„ embeddings
 */
export async function updatePostEmbeddings(post: Post) {
  console.log(`ğŸ”„ Updating embeddings for post ${post.id}`);

  // 1. åˆ é™¤æ—§æ•°æ®
  await deleteEmbeddingsByPostId(post.id);

  // 2. é‡æ–°ç”Ÿæˆ
  await generatePostEmbeddings(post);

  console.log(`âœ… Updated embeddings for post ${post.id}`);
}

/**
 * æœç´¢æ–‡ç« ï¼ˆå»é‡ + æ’åºï¼‰
 */
export async function searchPosts(
  query: string,
  options: {
    limit?: number;
    minSimilarity?: number;
  } = {}
) {
  const { limit = 10, minSimilarity = 0.7 } = options;

  // 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
  const queryEmbedding = await generateEmbedding(query);

  // 2. å‘é‡æœç´¢
  const results = await searchSimilarEmbeddings(queryEmbedding, {
    limit: limit * 2, // å¤šå–ä¸€äº›ï¼Œå› ä¸ºè¦å»é‡
    minSimilarity,
  });

  // 3. æŒ‰æ–‡ç« å»é‡ï¼ˆå¤šä¸ª chunk å¯èƒ½å±äºåŒä¸€ç¯‡ï¼‰
  const uniquePosts = new Map<string, (typeof results)[0]>();

  for (const result of results) {
    const existing = uniquePosts.get(result.post_id);

    // ä¿ç•™ç›¸ä¼¼åº¦æ›´é«˜çš„
    if (!existing || result.similarity > existing.similarity) {
      uniquePosts.set(result.post_id, result);
    }
  }

  // 4. æ’åºå¹¶é™åˆ¶æ•°é‡
  return Array.from(uniquePosts.values())
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, limit)
    .map((r) => ({
      postId: r.post_id,
      title: r.post_title,
      slug: r.post_slug,
      snippet: r.text_chunk.slice(0, 200) + "...",
      similarity: r.similarity,
      contentType: r.content_type,
    }));
}
